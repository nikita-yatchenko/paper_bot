{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19e2d89-7e71-4c6d-a26d-99c9a6ea8f59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f60d33-c0eb-4973-8cc2-f646ea653807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e749b151-2c38-4ce0-a268-04a9fb7a4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# Initialize the model\n",
    "model = AutoModel.from_pretrained('jinaai/jina-clip-v1', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f051e0ea-03a7-4083-b8b7-1f48b73094a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New meaningful sentences\n",
    "sentences = [\"this is a photo of cats\", \"this is a photo of dogs\"]\n",
    "sentences = ['A blue cat', 'A red cat']\n",
    "\n",
    "# Public image URLs\n",
    "images = ['./cats.jpg']\n",
    "images = [\n",
    "    'https://i.pinimg.com/600x315/21/48/7e/21487e8e0970dd366dafaed6ab25d8d8.jpg',\n",
    "    'https://i.pinimg.com/736x/c9/f2/3e/c9f23e212529f13f19bad5602d84b78b.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d058bf2c-753c-438f-91bf-88b0e723d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_dim = 512\n",
    "\n",
    "# Encode text and images\n",
    "text_embeddings = model.encode_text(sentences,\n",
    "                                    # truncate_dim=truncate_dim, \n",
    "                                    # normalize_embeddings=True\n",
    "                                   )\n",
    "image_embeddings = model.encode_image(\n",
    "    images,\n",
    "    # truncate_dim=truncate_dim,\n",
    "    # normalize_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "70a29e27-7374-4ef3-a31c-9890dfe1e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    return(np.exp(x)/np.exp(x).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "10d14086-480b-4216-8e02-5de6471913f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29071358, 0.05689978],\n",
       "       [0.12780233, 0.29157525]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(text_embeddings, image_embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bae68c67-291d-46ab-850c-4d4535ae5015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27457157, 0.23329428],\n",
       "       [0.21732593, 0.27480826]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.matmul(text_embeddings, image_embeddings.T).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8cdaecf7-9d68-46f6-bbd1-4743aa5ac2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1352822 ,  0.04169303,  0.05958096, ...,  0.03613621,\n",
       "         0.02080175,  0.00599309],\n",
       "       [-0.09709381, -0.00374282, -0.03742085, ...,  0.01427481,\n",
       "         0.0357096 , -0.01827639]], shape=(2, 512), dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14f8340b-6c49-40b0-b6c5-49180c075e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.65580558e-03,  1.03074878e-01, -3.40717249e-02,  1.07741021e-01,\n",
       "        3.00849341e-02,  4.10550609e-02, -6.21250235e-02, -2.76538767e-02,\n",
       "       -5.66998050e-02,  3.28176357e-02,  7.37224519e-02, -2.36659106e-02,\n",
       "       -1.39976025e-01,  5.32315448e-02, -2.64433213e-02,  1.31805792e-01,\n",
       "        1.11560049e-02,  1.33657247e-01,  5.81247807e-02, -4.43353169e-02,\n",
       "       -7.73518756e-02, -3.99740897e-02,  4.41878922e-02, -1.13537788e-01,\n",
       "        1.34383500e-01, -1.45269513e-01, -2.52276342e-02,  4.37926985e-02,\n",
       "       -5.81952371e-02,  1.61583096e-01, -6.08372241e-02, -2.61915606e-02,\n",
       "        1.21249352e-03, -2.37429470e-01, -1.08846202e-01,  9.57844555e-02,\n",
       "       -5.08496128e-02, -6.14304505e-02,  1.93413384e-02,  1.23099312e-01,\n",
       "        9.21819732e-03,  7.88313523e-02,  9.78529453e-02, -1.17411643e-01,\n",
       "       -2.67662015e-02,  9.07308795e-03,  3.54551189e-02, -1.62499830e-01,\n",
       "        3.24027874e-02, -5.61191561e-03, -1.00332305e-01,  4.27052937e-02,\n",
       "       -6.32279590e-02, -1.11896157e-01, -4.71029570e-03,  1.59882724e-01,\n",
       "       -1.06142662e-01,  6.84078410e-02, -1.76011249e-01, -2.28485376e-01,\n",
       "        2.62644645e-02,  1.09417528e-01,  5.39833270e-02, -2.11663216e-01,\n",
       "        1.38771283e-02, -2.99407486e-02,  1.55663434e-02,  1.28464643e-02,\n",
       "       -4.67333980e-02, -5.63317761e-02,  4.39673550e-02, -5.78927668e-03,\n",
       "        1.31549183e-02,  8.50556605e-03,  1.60097447e-03, -3.30203213e-02,\n",
       "       -1.78199485e-02, -8.43128115e-02,  3.94172780e-02, -5.20682782e-02,\n",
       "       -5.79928830e-02, -2.50842180e-02,  1.02577424e-02, -1.39932241e-02,\n",
       "       -7.96440393e-02, -4.65671383e-02, -9.25059244e-02, -3.02821808e-02,\n",
       "       -3.87439989e-02,  1.15724960e-02,  2.20922660e-02, -1.25051680e-04,\n",
       "       -5.20392284e-02, -2.37845397e-03, -3.66175926e-04,  3.83703746e-02,\n",
       "       -8.88502225e-02,  4.34549116e-02, -1.99349672e-02,  1.22354895e-01,\n",
       "        3.95458750e-02, -4.70207669e-02, -1.92335006e-02,  3.86411436e-02,\n",
       "       -1.68529544e-02,  5.92880091e-03,  9.59530026e-02,  1.88569576e-02,\n",
       "       -2.46195192e-03,  5.45002893e-02,  1.69646386e-02,  3.46188201e-03,\n",
       "       -4.06439453e-02,  3.16625796e-02, -4.80914209e-03,  7.83097558e-03,\n",
       "        1.50490715e-03, -1.83488540e-02,  9.08941124e-03, -3.80257517e-02,\n",
       "        6.14133254e-02,  1.15404027e-02,  9.64495912e-02, -1.85882896e-02,\n",
       "        2.76796408e-02,  1.64041929e-02, -2.38319561e-02, -2.32041697e-04,\n",
       "        3.58865485e-02,  9.97080002e-03,  1.87436789e-02, -4.25946852e-03,\n",
       "       -1.67099144e-02, -2.17632428e-02, -3.58021297e-02,  1.86950490e-02,\n",
       "        2.75475103e-02,  3.14744674e-02, -1.53375026e-02,  1.06328670e-02,\n",
       "        5.40592708e-02,  2.86075491e-02,  4.82245088e-02,  2.74225269e-02,\n",
       "       -8.98819789e-03, -3.52568924e-02,  4.97899130e-02,  1.63828465e-03,\n",
       "       -1.06776236e-02,  5.96959218e-02,  1.28904032e-02, -1.30230309e-02,\n",
       "       -3.50462720e-02, -7.57870674e-02,  4.17488702e-02, -2.06608009e-02,\n",
       "       -3.45202126e-02,  4.26134728e-02,  4.70194872e-03, -4.28106859e-02,\n",
       "        2.98192669e-02, -1.68203115e-02, -1.02083441e-02, -4.37681377e-02,\n",
       "       -4.87951636e-02,  2.24692319e-02, -5.14540961e-03, -1.95890199e-02,\n",
       "        3.39276753e-02,  1.87191702e-02,  1.61389038e-02, -2.91300081e-02,\n",
       "       -5.84981171e-03, -2.54540984e-02,  3.93242203e-02,  8.33073407e-02,\n",
       "       -4.47196104e-02,  9.08973664e-02, -2.76162731e-03,  1.80379569e-03,\n",
       "        1.23674545e-04, -1.11411382e-02, -7.72108510e-02,  6.72734156e-02,\n",
       "        8.83792713e-03, -5.00999996e-03,  2.08923519e-02, -1.23481629e-02,\n",
       "        6.77454546e-02,  2.02416275e-02,  3.01783402e-02, -3.36129963e-02,\n",
       "        4.34828103e-02, -5.38314134e-02,  1.27781546e-02,  3.44472528e-02,\n",
       "       -2.46036220e-02, -1.45957964e-02, -5.39628752e-02, -2.79985238e-02,\n",
       "       -1.72261763e-02,  7.84801692e-03, -5.68410754e-03,  7.75595754e-03,\n",
       "       -5.60840592e-02, -2.29117516e-02, -1.87751856e-02, -5.39841456e-03,\n",
       "        3.30922864e-02,  8.01100396e-03,  4.30485234e-03, -2.82485690e-02,\n",
       "       -3.23207229e-02,  1.07402746e-02, -2.94597987e-02, -1.15366820e-02,\n",
       "       -8.25991556e-02, -1.57545619e-02,  1.09520955e-02, -5.52830957e-02,\n",
       "       -7.50271678e-02, -3.38938981e-02,  1.52566165e-04, -4.49448898e-02,\n",
       "        1.70297921e-02,  1.42339161e-02,  2.99177002e-02,  2.80836467e-02,\n",
       "        4.25885851e-03, -4.06732783e-02,  2.83591896e-02,  5.96503578e-02,\n",
       "       -2.98729260e-02, -5.89681529e-02, -3.50267850e-02, -1.21700950e-02,\n",
       "       -2.72226371e-02,  1.78766977e-02, -6.32980168e-02, -2.11231746e-02,\n",
       "       -6.29386865e-03, -8.51487666e-02,  4.54083942e-02,  1.79996267e-02,\n",
       "        1.87594015e-02, -4.13014442e-02, -2.69431341e-02, -2.94718537e-02,\n",
       "       -3.85515712e-04, -1.48602547e-02, -7.01553747e-02,  4.07823175e-02,\n",
       "       -4.64705899e-02, -1.12221874e-02, -5.31766117e-02, -1.31574590e-02,\n",
       "        3.20436358e-02,  8.38853512e-03,  5.49806701e-03,  5.77701675e-03,\n",
       "       -2.91674454e-02,  3.30157727e-02,  2.24498622e-02,  6.79461169e-04,\n",
       "       -2.91749220e-02,  5.06084338e-02, -1.21803768e-02, -2.44392063e-02,\n",
       "        2.85619050e-02, -1.32181663e-02, -2.64694225e-02,  2.32148450e-02,\n",
       "        3.22834519e-03,  6.63509741e-02,  1.75334159e-02, -1.05404016e-02,\n",
       "        1.74049512e-02, -8.74730852e-03,  2.67041102e-02,  2.46577282e-02,\n",
       "        1.37032159e-02, -3.81171005e-04, -2.04152800e-02, -2.80329809e-02,\n",
       "        1.23758591e-03,  6.22323109e-03,  3.02592181e-02,  2.47710552e-02,\n",
       "       -2.56892070e-02,  4.92152125e-02,  2.01866552e-02, -3.02416459e-03,\n",
       "       -9.08092409e-03, -7.15480186e-04, -1.04358075e-02, -4.40522190e-03,\n",
       "       -3.27677540e-02, -6.56400770e-02, -2.69404124e-03, -2.30726562e-02,\n",
       "        2.21956037e-02,  3.68163630e-04, -1.84638184e-02,  1.79741643e-02,\n",
       "       -4.05510096e-03,  4.31349827e-03,  1.77409630e-02,  3.77248153e-02,\n",
       "        3.61306667e-02,  3.65318060e-02, -2.07182253e-03,  1.12559889e-02,\n",
       "        4.12165915e-04, -5.32059222e-02,  8.81263614e-03,  3.95013466e-02,\n",
       "        3.94307002e-02, -2.07925439e-02,  1.06431404e-02,  1.47963827e-02,\n",
       "       -2.81698387e-02, -6.04206994e-02, -2.73065250e-02, -1.60776768e-02,\n",
       "       -1.60908066e-02,  1.33456551e-02, -1.20703736e-02,  2.59283166e-02,\n",
       "        2.32341625e-02,  7.24543119e-03, -2.54211519e-02, -2.79520452e-02,\n",
       "        1.51608069e-03,  2.77812332e-02, -2.14456767e-02, -2.67188046e-02,\n",
       "       -2.27083676e-02,  1.60499569e-02,  1.28566101e-02, -5.06750867e-03,\n",
       "        3.82249728e-02,  9.12171137e-03,  3.30332257e-02, -1.67861453e-03,\n",
       "       -1.72402896e-02,  2.71308161e-02, -7.68865971e-03, -2.54341923e-02,\n",
       "       -1.62957574e-03,  4.44246791e-02,  3.17533826e-03, -5.77764772e-03,\n",
       "        2.21145302e-02, -3.22248489e-02, -1.05742365e-03,  2.36661592e-03,\n",
       "        2.67099217e-02, -3.58936517e-03,  1.25783095e-02,  2.97090374e-02,\n",
       "       -1.96871981e-02, -1.38545204e-02,  6.05314504e-04,  2.99214981e-02,\n",
       "       -2.42648777e-02,  1.85249243e-02,  1.73324365e-02, -2.55687907e-02,\n",
       "       -1.45097624e-03, -8.13464075e-03, -4.20297682e-03,  1.23030478e-02,\n",
       "        1.54467821e-02, -4.82540764e-02, -1.93636976e-02,  6.59870420e-05,\n",
       "       -6.80263564e-02, -2.26984546e-02, -4.81452839e-03, -4.32780124e-02,\n",
       "       -1.15722427e-02,  6.65429514e-03,  4.53205267e-03,  3.65565941e-02,\n",
       "       -2.10261047e-02, -3.26479748e-02,  2.79601421e-02, -3.66599597e-02,\n",
       "        1.10461004e-02,  3.39256786e-03, -3.65508167e-04,  4.41399534e-05,\n",
       "       -9.94884782e-03,  3.72242630e-02,  1.54446699e-02, -3.65623720e-02,\n",
       "        8.35725293e-03,  6.80739619e-03, -5.86333126e-03, -1.26207639e-02,\n",
       "        2.13382505e-02, -4.92011197e-03,  3.18489186e-02, -3.62255648e-02,\n",
       "        1.34204449e-02,  2.11123414e-02,  3.06850672e-02,  5.23463171e-03,\n",
       "        2.93505546e-02, -4.28948924e-03, -4.74789366e-03,  2.00047600e-03,\n",
       "        5.05970009e-02, -8.54807906e-03,  2.96837799e-02, -2.87770201e-02,\n",
       "        8.32726341e-03,  5.13318367e-03,  3.93103622e-02, -1.44687481e-02,\n",
       "       -4.06163521e-02,  2.22082026e-02,  1.91248320e-02,  1.43800937e-02,\n",
       "        1.11951251e-02,  1.98631007e-02, -4.80677709e-02, -1.99998673e-02,\n",
       "       -4.56828577e-03, -1.01927379e-02, -1.58079946e-03,  1.95123702e-02,\n",
       "       -1.53354630e-02,  5.03332429e-02,  1.25598721e-02, -3.55102345e-02,\n",
       "        1.31285004e-02, -1.14806425e-02,  1.83772780e-02,  2.91151635e-04,\n",
       "       -4.38443013e-02,  1.56064546e-02,  6.07060827e-03, -1.28497602e-02,\n",
       "       -1.41912242e-02,  1.01394337e-02, -2.47679316e-02, -9.47831292e-03,\n",
       "       -1.65858250e-02,  5.71357943e-02,  1.36733372e-02, -9.71169211e-03,\n",
       "       -1.79550163e-02, -1.67088043e-02, -5.19246496e-02,  2.53722146e-02,\n",
       "       -2.30287369e-02, -1.42643340e-02, -1.62592847e-02, -1.44856991e-02,\n",
       "       -8.41518678e-03, -4.22655791e-03, -1.30460262e-02,  1.34669002e-02,\n",
       "        4.01918143e-02,  6.77441107e-03, -9.06103570e-03, -4.31456380e-02,\n",
       "        4.65735458e-02,  2.17806753e-02,  6.73996226e-04, -1.10833636e-02,\n",
       "        5.75948041e-03, -2.02204622e-02, -3.59372050e-03, -1.47035588e-02,\n",
       "       -1.38750183e-03, -2.36905087e-02,  1.27073908e-02,  4.40832078e-02,\n",
       "        2.71907728e-02, -2.41198428e-02,  3.47529389e-02, -2.58373264e-02,\n",
       "       -2.74196547e-03, -2.27792636e-02, -1.25034796e-02, -4.96874051e-03,\n",
       "       -3.93149070e-03, -4.61393315e-03,  5.40381223e-02,  2.04377412e-03,\n",
       "        2.55142804e-02, -2.90131234e-02, -1.97517741e-02,  2.37283297e-02,\n",
       "        4.51322235e-02,  2.48657484e-02,  4.70482232e-03,  6.70330087e-03,\n",
       "        2.05384400e-02,  9.29868873e-03, -2.42469423e-02, -6.51406078e-03,\n",
       "        2.14671791e-02, -3.22331712e-02, -1.35962022e-02,  1.01289863e-03,\n",
       "       -2.70145666e-02,  5.08937892e-03, -1.56397559e-03, -1.03942526e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1689e6-c340-4484-8546-b74c5d5455c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0e2ba-fc42-43b2-9bf6-fd00d79a8a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e89c223a-fb29-4517-bc24-e1dd8203677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(DEVICE)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84ea45aa-12dc-4ca3-b421-b7cf4f6c2811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5274bde7-961d-4b95-ab86-74e605c14bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats.jpg', 'figures', '.ipynb_checkpoints', 'sandbox.ipynb', 'app.log']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "27887957-cff1-4ef8-8d70-dbb01d2a20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open('./cats.jpg')\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca561bd4-5fa4-4253-8c9a-4b847f2db849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c21a0722-ab25-4432-a2e4-482081858ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextTransformer(\n",
       "  (embeddings): CLIPTextEmbeddings(\n",
       "    (token_embedding): Embedding(49408, 768)\n",
       "    (position_embedding): Embedding(77, 768)\n",
       "  )\n",
       "  (encoder): CLIPEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x CLIPEncoderLayer(\n",
       "        (self_attn): CLIPSdpaAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f2484b9-33a1-43ce-bc0f-db5da45b61d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.text_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "703a6442-0dc8-48f1-b9d8-ad8a055e3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    outputs_text = model.text_projection(model.text_model(**inputs).pooler_output)\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    outputs_image = model.visual_projection(model.vision_model(**inputs).pooler_output)\n",
    "\n",
    "    inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1cfef358-5107-40bc-b83d-56b30d37b7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0553,  0.0492,  0.5154,  ...,  0.4025,  0.1555,  0.2346],\n",
       "        [ 0.1627, -0.0700,  0.3643,  ...,  0.5557,  0.0457,  0.1955]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc65067f-92b5-423b-9dd4-7fa06c9b44a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0031,  0.0028,  0.0291,  ...,  0.0227,  0.0088,  0.0132],\n",
       "        [ 0.0091, -0.0039,  0.0203,  ...,  0.0310,  0.0025,  0.0109]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44b349a4-9498-4c44-99c5-fa6f18217598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00],\n",
       "        [3.4962e-11]], device='mps:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(outputs_text, outputs_image.T).softmax(dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "850acd95-4102-4585-8e78-f7832af99bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9925e-01, 7.5487e-04]], device='mps:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits_per_image.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1bd05db-ad43-449e-b54f-1fbfe5fb76b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9925e-01, 7.5487e-04]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b10130c-047f-4e76-b4e9-9f6cda998a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else DEVICE\n",
    "\n",
    "# attention_type = \"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\"\n",
    "# processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "# model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "#                                                torch_dtype=torch.bfloat16,\n",
    "#                                                _attn_implementation=attention_type).to(DEVICE)\n",
    "\n",
    "# model_path = 'HuggingFaceTB/SmolVLM-Instruct'\n",
    "# quant_path = 'qwen-instruct-v0.2-awq'\n",
    "# quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "#\n",
    "# # Загружаем модель\n",
    "# model = AutoAWQForCausalLM.from_pretrained(\n",
    "#     model_path, **{\"low_cpu_mem_usage\": True, \"use_cache\": False}\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "#\n",
    "# # Квантизация\n",
    "# model.quantize(tokenizer, quant_config=quant_config)\n",
    "#\n",
    "# # Сохраняем квантованную модель\n",
    "# model.save_quantized(quant_path)\n",
    "# tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f001ce63-3a86-481b-b0d4-0e743d40e168",
   "metadata": {},
   "source": [
    "# Paper Processor pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b29f4a1-228c-4952-8d20-1f34a89ae244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9026fca-b894-45e6-8604-0927f2f59b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.services.file_processor.client import PaperProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47757d8a-9ef9-4f8f-a192-5b9f1dcc21dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 17:55:26,847 - paper_bot - INFO - Loading model\n",
      "2025-04-21 17:55:28,605 - paper_bot - INFO - Loading pretrained summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "pp = PaperProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a115dcfd-c726-40f9-b547-2039d6390588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 17:55:39,682 - paper_bot - INFO - PDF elements extraction 2307.00651v1\n",
      "2025-04-21 17:56:05,872 - paper_bot - DEBUG - PDF elements extracted. Raw elements: 11\n",
      "2025-04-21 17:56:05,873 - paper_bot - INFO - Starting categorizing elements...\n",
      "2025-04-21 17:56:05,874 - paper_bot - DEBUG - PDF elements extracted. Text: 11. Tables: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (951 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 17:57:13,848 - paper_bot - DEBUG - Adding texts: 15\n",
      "2025-04-21 17:57:13,849 - paper_bot - DEBUG - Adding text summaries: 15\n",
      "2025-04-21 17:57:13,850 - paper_bot - DEBUG - Adding summaries: SSL is now a serious competitor for supervised learning, even though it does not require data annotation . There is no clear consensus on whether maximizing or minimizing the mutual information between representations of augmentation views practically contribute to improvement or degradation in performance of SSL models . This paper is a fundamen- tal work where, we investigate role of mutual information in SSL .\n",
      "2025-04-21 17:57:13,850 - paper_bot - DEBUG - Docs: 15\n"
     ]
    }
   ],
   "source": [
    "await pp.process('2307.00651v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349e6648-2a49-486f-8002-30db8b173c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_all_documents(retriever):\n",
    "#     \"\"\"\n",
    "#     Retrieve all documents stored in the MultiVectorRetriever.\n",
    "\n",
    "#     Args:\n",
    "#         retriever: An instance of MultiVectorRetriever.\n",
    "\n",
    "#     Returns:\n",
    "#         List[Dict]: A list of dictionaries containing document IDs and their content.\n",
    "#     \"\"\"\n",
    "#     # Fetch all document IDs from the vector store\n",
    "#     doc_ids = retriever.vectorstore.get(include=[\"embeddings\"])[\"ids\"]\n",
    "\n",
    "#     # Retrieve documents from the document store using the IDs\n",
    "#     documents = []\n",
    "#     for doc_id in doc_ids:\n",
    "#         doc_content = retriever.docstore.mget([doc_id])[0]\n",
    "#         documents.append({\"id\": doc_id, \"content\": doc_content})\n",
    "\n",
    "#     return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c84ec6dd-1cb5-407d-b6e0-a2e1570a1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_all_documents(pp.retrievers[\"2307.00651v1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "010e5dcc-c01f-4c4d-993d-66014477d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp.retrievers[\"2307.00651v1\"].vectorstore.get(ids=['256a39f0-2355-44ab-afb7-702b38da0bca'], include=[\"documents\"])[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e71f577-fff8-4d52-8d61-8bd592a3a1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiVectorRetriever(vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x30fa234d0>, docstore=<langchain_core.stores.InMemoryStore object at 0x33746cb30>, search_kwargs={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.retrievers[\"2307.00651v1\"]#(\"explain the main method authors proposed in the paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbb2e24c-d09b-43b5-bbe2-1ec335c27c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 18:01:06,339 - paper_bot - DEBUG - Found texts: 4\n",
      "Question: what is the paper about?\n",
      "Retrieved Context:\n",
      "Document 1:\n",
      "Content: In this paper, we assess how this whitening process un- wittingly eliminates the synergistic information along with redundant information . This eventually leads us to reconsider the problem of mutual information between two variables (two views and the target representation) This leads to a larger controversy on how mutual information relates to learning the target rep- resentation .\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "Content: In its simplest form, suppose we have two source variables S1 and S2 carrying joint mutual information I(T;S1,S2) about a target variable T . Hence each of the source variables has mutual information with the target variable . The joint information between sources and target could be decomposed as three elements, unique, redundant, and synergistic information . This contrast in essence creates an information interaction between the information of the variables which could be studied under the PID framework .\n",
      "--------------------------------------------------\n",
      "Document 3:\n",
      "Content: [12] Philip Bachman, R Devon Hjelm, and William Buchwalter, “Learning represen- tations by maximizing mutual information across views,” Advances in neural information processing systems, vol. 32, 2019. [13] Jean-Bastien Grill, Florian Strub, Florent Altch e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad G\n",
      "--------------------------------------------------\n",
      "Document 4:\n",
      "Content: Decomposing Joint Mutual Information For the first time ever we consider the general SSL problem setting from the viewpoint of PID, which has diverse practical applications including in neuroscience, game theory and sta- tistical learning . This new interpretation of SSL is primarily posed to address the ambiguity in the role of mutual information in SSL .\n",
      "--------------------------------------------------\n",
      "2025-04-21 18:01:06,341 - paper_bot - INFO - Formatted texts: 1633\n",
      "2025-04-21 18:01:06,344 - paper_bot - DEBUG - Processed prompt: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The paper is about assessing how the whitening process un-wittingly eliminates synergistic information along with redundant information, which eventually leads to a larger controversy on how mutual information relates to learning the target representation.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.respond(\"what is the paper about?\", \"2307.00651v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea793f91-b308-487e-8193-99bfbad78de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.services.vlm.client import CustomLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ecc04-875b-4282-b3bb-4acc2da43e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CustomLLM()\n",
    "q_and_context = {\n",
    "    \"context\": {\"texts\": [\"no information found\"], \"images\": []},\n",
    "    \"question\": \"what does the world look like today?\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7527215a-21f8-4457-9e56-4b051943feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 16:25:55,812 - paper_bot - INFO - Formatted texts: 20\n"
     ]
    }
   ],
   "source": [
    "processed = llm.preprocess_input(q_and_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2fd28f5-107d-4049-9ec9-31c187a91543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = len(processed['input_ids'][0])\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a798925a-261e-43ce-b6d9-a1a1e1bd9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k: v.to(llm.device) for k, v in processed.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebaa66cb-23f9-40aa-85ce-3ca685cdf4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 11126,    42,  2988,  2914,  1962,    42,   732,  1072,   260,\n",
       "           905,  1492,   702,  1834,    47,   198,   198, 11247,  1015, 44339,\n",
       "          1096,   281,  1694,   284,  2272,   355,  8158,  2441,    42,   198,\n",
       "          4607,  1096,   983, 49154,   198,  9519,  9531,    42]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bde730ca-9be2-407b-b5fe-1ce18e39158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = llm.model.generate(**inputs, max_new_tokens=128, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3f1da66-b85e-4ad2-b01e-56e20e9b23ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 61])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "607c4076-a46d-4f0c-96da-57382b20aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_gen_id = generated_ids[0][l:].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e136bf11-6a07-4acc-a9a0-73977408a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = llm.processor.batch_decode(\n",
    "    mod_gen_id,\n",
    "    skip_special_tokens=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1e5e854-5b6c-4c7c-ba06-ff053455519f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The world today is a place where people live in different countries, speak different languages, and have different cultures.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe2616-a37f-4c88-a3a0-3359b0c78d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd50f6c-7095-4eca-adc5-e7425b4c26e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "465f7b7c-7dbe-4a08-ad66-e602624dd0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2431817-fae2-4ef4-adf5-dd7fa30c72d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6a8fae1-bbfc-4a2b-8ce0-3592e94bec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'SSL frameworks consist of two key elements, namely, loss function, and pretext task [8] This type of baselines suffer from the problem of potential representation collapse, as well as the need for large negative batches for effective representation . Next generation baselines emerged as non-contrastive or negative pair-free baselines [13, 14], essentially eliminating the need to contrast against negative views (negative pairs), and almost with no risk of representation collapse .',\n",
    "    'SSL frameworks consist of two key elements, namely, loss function, and pretext task [8] This type of baselines suffer from the problem of potential representation collapse, as well as the need for large negative batches for effective representation . Next generation baselines emerged as non-contrastive or negative pair-free baselines [13, 14], essentially eliminating the need to contrast against negative views (negative pairs), and almost with no risk of representation collapse .',\n",
    "    'SSL frameworks consist of two key elements, namely, loss function, and pretext task [8] This type of baselines suffer from the problem of potential representation collapse, as well as the need for large negative batches for effective representation . Next generation baselines emerged as non-contrastive or negative pair-free baselines [13, 14], essentially eliminating the need to contrast against negative views (negative pairs), and almost with no risk of representation collapse .',\n",
    "    'SSL frameworks consist of two key elements, namely, loss function, and pretext task [8] This type of baselines suffer from the problem of potential representation collapse, as well as the need for large negative batches for effective representation . Next generation baselines emerged as non-contrastive or negative pair-free baselines [13, 14], essentially eliminating the need to contrast against negative views (negative pairs), and almost with no risk of representation collapse .',\n",
    "    'SSL frameworks consist of two key elements, namely, loss function, and pretext task [8] This type of baselines suffer from the problem of potential representation collapse, as well as the need for large negative batches for effective representation . Next generation baselines emerged as non-contrastive or negative pair-free baselines [13, 14], essentially eliminating the need to contrast against negative views (negative pairs), and almost with no risk of representation collapse .',\n",
    "    'SSL frameworks consist of two key elements, namely, loss function, and pretext task [8] This type of baselines suffer from the problem of potential representation collapse, as well as the need for large negative batches for effective representation . Next generation baselines emerged as non-contrastive or negative pair-free baselines [13, 14], essentially eliminating the need to contrast against negative views (negative pairs), and almost with no risk of representation collapse .',\n",
    "    'SSL frameworks consist of two key elements, namely, loss function, and pretext task [8] This type of baselines suffer from the problem of potential representation collapse, as well as the need for large negative batches for effective representation . Next generation baselines emerged as non-contrastive or negative pair-free baselines [13, 14], essentially eliminating the need to contrast against negative views (negative pairs), and almost with no risk of representation collapse .',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0943b78e-e32c-492f-936d-3ce900a15bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 500, but your input_length is only 101. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 500, but your input_length is only 101. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    }
   ],
   "source": [
    "texts_summaries = summarizer(\n",
    "                texts,  # List of input texts\n",
    "                max_length=500,  # Maximum length of the summary\n",
    "                min_length=25,  # Minimum length of the summary\n",
    "                do_sample=False,  # Use deterministic summarization\n",
    "                batch_size=4  # Number of texts processed in parallel\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08ccf463-868d-4a72-9297-4cf8313ef9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_summaries), len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ab10ba-f731-4590-903a-361454da2a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'SSL frameworks consist of two key elements: loss function and pretext task . Next generation baselines emerged as non-contrastive or negative pair-free baselines . This type of baselines suffer from the problem of potential representation collapse .'},\n",
       " {'summary_text': 'SSL frameworks consist of two key elements: loss function and pretext task . Next generation baselines emerged as non-contrastive or negative pair-free baselines . This type of baselines suffer from the problem of potential representation collapse .'},\n",
       " {'summary_text': 'SSL frameworks consist of two key elements: loss function and pretext task . Next generation baselines emerged as non-contrastive or negative pair-free baselines . This type of baselines suffer from the problem of potential representation collapse .'},\n",
       " {'summary_text': 'SSL frameworks consist of two key elements: loss function and pretext task . Next generation baselines emerged as non-contrastive or negative pair-free baselines . This type of baselines suffer from the problem of potential representation collapse .'},\n",
       " {'summary_text': 'SSL frameworks consist of two key elements: loss function and pretext task . Next generation baselines emerged as non-contrastive or negative pair-free baselines . This type of baselines suffer from the problem of potential representation collapse .'},\n",
       " {'summary_text': 'SSL frameworks consist of two key elements: loss function and pretext task . Next generation baselines emerged as non-contrastive or negative pair-free baselines . This type of baselines suffer from the problem of potential representation collapse .'},\n",
       " {'summary_text': 'SSL frameworks consist of two key elements: loss function and pretext task . Next generation baselines emerged as non-contrastive or negative pair-free baselines . This type of baselines suffer from the problem of potential representation collapse .'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b4c26-5d90-44db-8a94-04646734e3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
